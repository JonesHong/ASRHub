#!/usr/bin/env python3
"""
VAD æ•´åˆæ¸¬è©¦å·¥å…·
æ¸¬è©¦ SileroVADOperator çš„åŠŸèƒ½å’Œæº–ç¢ºæ€§
"""

import asyncio
import sys
import os
import time
import numpy as np
import pyaudio
import threading
from typing import Dict, Any, Optional, List
from pathlib import Path

# æ·»åŠ å°ˆæ¡ˆæ ¹ç›®éŒ„åˆ° Python è·¯å¾‘
sys.path.insert(0, os.path.join(os.path.dirname(os.path.abspath(__file__)), '../../..'))

from src.pipeline.operators.vad import SileroVADOperator
from src.pipeline.operators.vad.events import VADEvent, VADEventData
from src.pipeline.operators.vad.statistics import VADFrame, VADStatisticsCollector
from src.pipeline.operators.audio_format.scipy_operator import ScipyAudioFormatOperator
from src.models.audio_format import AudioMetadata, AudioFormat
from src.utils.logger import logger
from src.utils.visualization import VADVisualization


class VADIntegrationTester:
    """VAD åŠŸèƒ½æ•´åˆæ¸¬è©¦"""
    
    def __init__(self):
        # åˆå§‹åŒ– VAD operator
        self.vad_operator = SileroVADOperator()
        
        # æ›´æ–°é…ç½®ä»¥é©åˆæ¸¬è©¦
        self.vad_operator.update_config({
            'threshold': 0.5,  # æ¨™æº–é–€æª»å€¼
            'min_silence_duration': 0.3,
            'min_speech_duration': 0.1,
            'adaptive_threshold': True,
            'smoothing_window': 3
        })
        
        # éŸ³è¨Šåƒæ•¸
        self.sample_rate = 16000
        self.channels = 1
        self.chunk_size = 512  # Silero VAD éœ€è¦ 512 æ¨£æœ¬
        self.format = pyaudio.paInt16
        
        # PyAudio
        self.p = pyaudio.PyAudio()
        self.stream = None
        self.is_running = False
        
        # è³‡æ–™æ”¶é›†
        self.vad_results = []
        self.statistics_collector = VADStatisticsCollector()
        self.statistics_collector.start_time = time.time()
        
        # è¦–è¦ºåŒ–
        self.visualization = VADVisualization()
        
    async def setup(self):
        """è¨­å®šæ¸¬è©¦ç’°å¢ƒ"""
        logger.info("è¨­å®š VAD æ¸¬è©¦ç’°å¢ƒ...")
        
        try:
            # åˆå§‹åŒ– VAD operator
            await self.vad_operator.start()
            
            # è¨­å®šäº‹ä»¶å›èª¿
            self.vad_operator.set_speech_callbacks(
                start_callback=self._on_speech_start,
                end_callback=self._on_speech_end,
                result_callback=self._on_vad_result
            )
            
            logger.info("âœ“ VAD æ¸¬è©¦ç’°å¢ƒè¨­å®šå®Œæˆ")
            
        except Exception as e:
            logger.error(f"è¨­å®šå¤±æ•—: {e}")
            raise
    
    async def cleanup(self):
        """æ¸…ç†æ¸¬è©¦ç’°å¢ƒ"""
        logger.info("æ¸…ç†æ¸¬è©¦ç’°å¢ƒ...")
        
        try:
            # å…ˆåœæ­¢è™•ç†å¾ªç’°
            self.is_running = False
            
            # ç­‰å¾…ä¸€å°æ®µæ™‚é–“è®“ç·šç¨‹çµæŸ
            await asyncio.sleep(0.1)
            
            # åœæ­¢éŸ³è¨Šæµ
            if self.stream:
                self.stream.stop_stream()
                self.stream.close()
                self.stream = None
            
            # æ¸…ç† PyAudio
            if self.p:
                self.p.terminate()
            
            # åœæ­¢ VAD operator
            if self.vad_operator:
                await self.vad_operator.stop()
            
            logger.info("âœ“ æ¸¬è©¦ç’°å¢ƒæ¸…ç†å®Œæˆ")
            
        except Exception as e:
            logger.error(f"æ¸…ç†éŒ¯èª¤: {e}")
    
    async def test_realtime(self):
        """å³æ™‚éŸ³è¨Šæ¸¬è©¦"""
        logger.info("é–‹å§‹å³æ™‚éŸ³è¨Š VAD æ¸¬è©¦")
        
        # é–‹å•ŸéŸ³è¨Šæµ
        try:
            self.stream = self.p.open(
                format=self.format,
                channels=self.channels,
                rate=self.sample_rate,
                input=True,
                frames_per_buffer=self.chunk_size
            )
            logger.info(f"éŸ³è¨Šæµå·²é–‹å•Ÿ: {self.sample_rate}Hz, {self.channels}ch")
        except Exception as e:
            logger.error(f"ç„¡æ³•é–‹å•ŸéŸ³è¨Šæµ: {e}")
            # å˜—è©¦å…¶ä»–æ¡æ¨£ç‡
            for rate in [44100, 48000, 8000]:
                try:
                    self.sample_rate = rate
                    self.stream = self.p.open(
                        format=self.format,
                        channels=self.channels,
                        rate=self.sample_rate,
                        input=True,
                        frames_per_buffer=self.chunk_size
                    )
                    logger.info(f"ä½¿ç”¨å‚™ç”¨æ¡æ¨£ç‡: {rate}Hz")
                    break
                except:
                    continue
        
        self.is_running = True
        
        # å•Ÿå‹•éŸ³è¨Šè™•ç†ç·šç¨‹
        audio_thread = threading.Thread(target=self._audio_processing_loop)
        audio_thread.daemon = True
        audio_thread.start()
        
        # å•Ÿå‹•è¦–è¦ºåŒ–
        await self._start_visualization()
    
    def _audio_processing_loop(self):
        """éŸ³è¨Šè™•ç†è¿´åœˆï¼ˆåœ¨ç·šç¨‹ä¸­é‹è¡Œï¼‰"""
        logger.info("éŸ³è¨Šè™•ç†ç·šç¨‹å•Ÿå‹•")
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        
        while self.is_running:
            try:
                # è®€å–éŸ³è¨Š
                audio_data = self.stream.read(self.chunk_size, exception_on_overflow=False)
                
                # è™•ç†éŸ³è¨Š
                loop.run_until_complete(self._process_audio_chunk(audio_data))
                
            except Exception as e:
                if self.is_running:
                    logger.error(f"éŸ³è¨Šè™•ç†éŒ¯èª¤: {e}")
                    time.sleep(0.01)
        
        loop.close()
    
    async def _process_audio_chunk(self, audio_data: bytes):
        """è™•ç†å–®å€‹éŸ³è¨Šå¡Š"""
        # è½‰æ›ç‚º numpy array
        audio_np = np.frombuffer(audio_data, dtype=np.int16)
        
        # å‰µå»ºå…ƒæ•¸æ“š
        metadata = AudioMetadata(
            sample_rate=self.sample_rate,
            channels=self.channels,
            format=AudioFormat.INT16
        )
        
        # åŸ·è¡Œ VAD
        result = await self.vad_operator.process(audio_data, metadata=metadata)
        
        # ç²å– VAD ç‹€æ…‹
        vad_state = self.vad_operator.get_info()
        
        # å°‡è³‡æ–™åŠ å…¥è¦–è¦ºåŒ–ä½‡åˆ—
        self.visualization.add_data({
            'audio': audio_np,
            'vad_state': vad_state,
            'timestamp': time.time(),
            'speech_prob': vad_state.get('speech_probability', 0),
            'threshold': self.vad_operator.threshold
        })
    
    async def _on_speech_start(self, event_data: Dict[str, Any]):
        """èªéŸ³é–‹å§‹äº‹ä»¶"""
        logger.info(f"ğŸ¤ èªéŸ³é–‹å§‹ (æ©Ÿç‡: {event_data.get('speech_probability', 0):.3f})")
    
    async def _on_speech_end(self, event_data: Dict[str, Any]):
        """èªéŸ³çµæŸäº‹ä»¶"""
        duration = event_data.get('speech_duration', 0)
        logger.info(f"ğŸ”‡ èªéŸ³çµæŸ (æ™‚é•·: {duration:.3f}s)")
    
    async def _on_vad_result(self, vad_result: Dict[str, Any]):
        """VAD çµæœäº‹ä»¶"""
        # æ”¶é›†çµ±è¨ˆ
        frame = VADFrame(
            timestamp=vad_result['timestamp'],
            speech_probability=vad_result['speech_probability'],
            is_speech=vad_result['speech_detected'],
            threshold=self.vad_operator.threshold
        )
        self.statistics_collector.add_frame(frame)
    
    async def _start_visualization(self):
        """å•Ÿå‹•è¦–è¦ºåŒ–"""
        logger.info("å•Ÿå‹• VAD è¦–è¦ºåŒ–ç›£æ§...")
        
        # è¨­å®šåœ–è¡¨
        self.visualization.setup_plot()
        
        # å•Ÿå‹•å‹•ç•«
        self.visualization.start_animation(self._update_plot, interval=100)
    
    def _update_plot(self, frame):
        """æ›´æ–°åœ–è¡¨"""
        # ç²å–æœ€æ–°æ•¸æ“š
        latest_data = self.visualization.get_latest_data()
        
        if latest_data:
            # æ›´æ–°éŸ³è¨Šæ³¢å½¢
            audio_data = latest_data['audio']
            self.visualization.update_audio_plot(audio_data)
            
            # æ›´æ–° VAD åœ–è¡¨
            vad_prob = latest_data['speech_prob']
            timestamp = latest_data['timestamp']
            threshold = latest_data.get('threshold', self.vad_operator.threshold)
            self.visualization.update_vad_plot(vad_prob, timestamp, threshold)
            
            # æ›´æ–°çµ±è¨ˆ
            stats = self.statistics_collector.get_statistics()
            recent_stats = self.statistics_collector.get_recent_statistics(window_seconds=10)
            
            # ç²å– VAD ç‹€æ…‹
            vad_state = latest_data.get('vad_state', {})
            
            # ä½¿ç”¨ç°¡æ½”çš„æ ¼å¼
            speech_ratio = stats.speech_frames / max(1, stats.total_frames)
            is_speaking = '[èªªè©±ä¸­]' if vad_state.get('in_speech', False) else '[éœéŸ³]'
            
            # è¨ˆç®—ç´¯ç©æ™‚é•·
            total_duration = time.time() - self.statistics_collector.start_time
            speech_duration = stats.total_speech_duration
            silence_duration = total_duration - speech_duration
            
            stats_text = (
                f"è™•ç†: {stats.total_frames} å¹€ | èªéŸ³: {stats.speech_frames} ({speech_ratio:.1%}) | "
                f"æœ€è¿‘10ç§’: {recent_stats.get('speech_ratio', 0):.1%}\n"
                f"{is_speaking} | èªéŸ³: {self.visualization.format_time(speech_duration)} | "
                f"éœéŸ³: {self.visualization.format_time(silence_duration)} | "
                f"é–¾å€¼: {threshold:.3f}"
            )
            
            if hasattr(self.visualization, 'texts') and 'stats' in self.visualization.texts:
                self.visualization.texts['stats'].set_text(stats_text)
        
        return []
    
    def print_test_results(self):
        """æ‰“å°æ¸¬è©¦çµæœ"""
        stats = self.statistics_collector.get_statistics()
        
        print("\n" + "="*60)
        print("ğŸ“Š VAD æ¸¬è©¦çµæœ")
        print("="*60)
        
        print(f"\nåŸºæœ¬çµ±è¨ˆ:")
        print(f"  ç¸½è™•ç†å¹€æ•¸: {stats.total_frames}")
        print(f"  èªéŸ³å¹€æ•¸: {stats.speech_frames}")
        print(f"  éœéŸ³å¹€æ•¸: {stats.silence_frames}")
        print(f"  èªéŸ³æ¯”ä¾‹: {stats.speech_frames / max(1, stats.total_frames):.2%}")
        
        print(f"\nèªéŸ³æ®µè½:")
        print(f"  æ®µè½æ•¸é‡: {len(stats.speech_segments)}")
        if stats.speech_segments:
            print(f"  ç¸½èªéŸ³æ™‚é•·: {stats.total_speech_duration:.3f}s")
            print(f"  å¹³å‡æ®µè½æ™‚é•·: {stats.average_segment_duration:.3f}s")
            print(f"  æœ€é•·æ®µè½: {stats.max_segment_duration:.3f}s")
            print(f"  æœ€çŸ­æ®µè½: {stats.min_segment_duration:.3f}s")
        
        print(f"\nè™•ç†æ•ˆèƒ½:")
        if stats.processing_times:
            print(f"  å¹³å‡è™•ç†æ™‚é–“: {stats.avg_processing_time * 1000:.3f}ms")
            print(f"  æœ€å¤§è™•ç†æ™‚é–“: {stats.max_processing_time * 1000:.3f}ms")
        
        print("="*60)


async def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸ¯ VAD æ•´åˆæ¸¬è©¦å·¥å…·")
    print("è«‹å°è‘—éº¥å…‹é¢¨èªªè©±ï¼Œè§€å¯Ÿ VAD æª¢æ¸¬æ•ˆæœ")
    print("æŒ‰ Ctrl+C æˆ–é—œé–‰è¦–çª—çµæŸæ¸¬è©¦")
    
    tester = VADIntegrationTester()
    
    try:
        # è¨­å®šæ¸¬è©¦ç’°å¢ƒ
        await tester.setup()
        
        # åŸ·è¡Œå³æ™‚æ¸¬è©¦
        await tester.test_realtime()
        
    except KeyboardInterrupt:
        print("\næ¸¬è©¦è¢«ç”¨æˆ¶ä¸­æ–·")
    except Exception as e:
        print(f"\næ¸¬è©¦éŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()
    finally:
        # æ¸…ç†è³‡æº
        await tester.cleanup()
        
        # æ‰“å°çµæœ
        tester.print_test_results()


if __name__ == "__main__":
    asyncio.run(main())