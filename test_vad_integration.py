#!/usr/bin/env python3
"""
VAD æ•´åˆæ¸¬è©¦å·¥å…·
æ¸¬è©¦ SileroVADOperator çš„åŠŸèƒ½å’Œæº–ç¢ºæ€§
"""

import asyncio
import sys
import os
import time
import numpy as np
import pyaudio
import threading
from typing import Dict, Any, Optional, List
from pathlib import Path

# æ·»åŠ å°ˆæ¡ˆæ ¹ç›®éŒ„åˆ° Python è·¯å¾‘
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from src.pipeline.operators.vad import SileroVADOperator
from src.pipeline.operators.vad.events import VADEvent, VADEventData
from src.pipeline.operators.vad.statistics import VADFrame, VADStatisticsCollector
from src.pipeline.operators.audio_format.scipy_operator import ScipyAudioFormatOperator
from src.models.audio_format import AudioMetadata, AudioFormat
from src.utils.logger import logger
from src.utils.visualization import VADVisualization

# logger = logger


class VADIntegrationTester:
    """VAD åŠŸèƒ½æ•´åˆæ¸¬è©¦"""
    
    def __init__(self):
        # åˆå§‹åŒ– VAD operator
        self.vad_operator = SileroVADOperator()
        
        # æ›´æ–°é…ç½®ä»¥é©åˆæ¸¬è©¦
        self.vad_operator.update_config({
            'threshold': 0.08,  # å¤§å¹…é™ä½é–€æª»å€¼ä»¥é©æ‡‰åˆæˆèªéŸ³
            'min_silence_duration': 0.2,
            'min_speech_duration': 0.05,
            'adaptive_threshold': False,  # é—œé–‰è‡ªé©æ‡‰é–¾å€¼
            'smoothing_window': 2
        })
        
        # éŸ³è¨Šæ ¼å¼è½‰æ›å™¨
        # VAD éœ€è¦çš„æ ¼å¼
        self.vad_format = AudioMetadata(
            sample_rate=16000,
            channels=1,
            format=AudioFormat.INT16
        )
        self.format_converter = ScipyAudioFormatOperator(
            operator_id='vad_converter',
            target_metadata=self.vad_format
        )
        
        # éº¥å…‹é¢¨è¼¸å…¥åƒæ•¸ï¼ˆç³»çµ±å¯¦éš›æ ¼å¼ï¼‰
        self.mic_sample_rate = 48000  # 48kHz
        self.mic_channels = 2         # ç«‹é«”è²
        self.mic_format = pyaudio.paInt16  # PyAudio ä½¿ç”¨ 16ä½å…ƒ
        
        # VAD è™•ç†åƒæ•¸
        self.sample_rate = 16000
        self.channels = 1
        self.chunk_size = 512  # Silero VAD éœ€è¦ 512 æ¨£æœ¬
        self.format = pyaudio.paInt16
        
        # è¨ˆç®—éº¥å…‹é¢¨çš„ chunk å¤§å°ï¼ˆè€ƒæ…®æ¡æ¨£ç‡å·®ç•°ï¼‰
        self.mic_chunk_size = int(self.chunk_size * self.mic_sample_rate / self.sample_rate)
        logger.info(f"éŸ³è¨Šåƒæ•¸: VAD chunk_size={self.chunk_size}, éº¥å…‹é¢¨ chunk_size={self.mic_chunk_size}")
        
        # PyAudio
        self.p = pyaudio.PyAudio()
        self.stream = None
        self.is_running = False
        
        # è³‡æ–™æ”¶é›†
        self.vad_results = []
        self.statistics_collector = VADStatisticsCollector()
        self.statistics_collector.start_time = time.time()
        
        # è¦–è¦ºåŒ–
        self.visualization = VADVisualization()
        
        # æ¸¬è©¦æ¨¡å¼
        self.test_mode = None  # 'realtime', 'file', 'synthetic'
        
    async def setup(self):
        """è¨­å®šæ¸¬è©¦ç’°å¢ƒ"""
        logger.info("è¨­å®š VAD æ¸¬è©¦ç’°å¢ƒ...")
        
        try:
            # åˆå§‹åŒ–æ ¼å¼è½‰æ›å™¨
            await self.format_converter.start()
            
            # åˆå§‹åŒ– VAD operator
            await self.vad_operator.start()
            
            # è¨­å®šäº‹ä»¶å›èª¿
            self.vad_operator.set_speech_callbacks(
                start_callback=self._on_speech_start,
                end_callback=self._on_speech_end,
                result_callback=self._on_vad_result
            )
            
            logger.info("âœ“ VAD æ¸¬è©¦ç’°å¢ƒè¨­å®šå®Œæˆ")
            
        except Exception as e:
            logger.error(f"è¨­å®šå¤±æ•—: {e}")
            raise
    
    async def cleanup(self):
        """æ¸…ç†æ¸¬è©¦ç’°å¢ƒ"""
        logger.info("æ¸…ç†æ¸¬è©¦ç’°å¢ƒ...")
        
        try:
            # å…ˆåœæ­¢è™•ç†å¾ªç’°
            self.is_running = False
            
            # ç­‰å¾…ä¸€å°æ®µæ™‚é–“è®“ç·šç¨‹çµæŸ
            await asyncio.sleep(0.1)
            
            # åœæ­¢éŸ³è¨Šæµ
            if self.stream:
                self.stream.stop_stream()
                self.stream.close()
                self.stream = None
            
            # æ¸…ç† PyAudio
            if self.p:
                self.p.terminate()
            
            # åœæ­¢ VAD operator
            if self.vad_operator:
                await self.vad_operator.stop()
            
            # åœæ­¢æ ¼å¼è½‰æ›å™¨
            if self.format_converter:
                await self.format_converter.stop()
            
            logger.info("âœ“ æ¸¬è©¦ç’°å¢ƒæ¸…ç†å®Œæˆ")
            
        except Exception as e:
            logger.error(f"æ¸…ç†éŒ¯èª¤: {e}")
    
    async def test_realtime(self):
        """å³æ™‚éŸ³è¨Šæ¸¬è©¦"""
        logger.info("é–‹å§‹å³æ™‚éŸ³è¨Š VAD æ¸¬è©¦")
        self.test_mode = 'realtime'
        
        # é–‹å•ŸéŸ³è¨Šæµï¼ˆä½¿ç”¨éº¥å…‹é¢¨å¯¦éš›åƒæ•¸ï¼‰
        logger.info(f"é–‹å•ŸéŸ³è¨Šæµ: format={self.mic_format}, channels={self.mic_channels}, rate={self.mic_sample_rate}, buffer={self.mic_chunk_size}")
        self.stream = self.p.open(
            format=self.mic_format,
            channels=self.mic_channels,
            rate=self.mic_sample_rate,
            input=True,
            frames_per_buffer=self.mic_chunk_size
        )
        logger.info(f"éŸ³è¨Šæµå·²é–‹å•Ÿ: is_active={self.stream.is_active()}")
        
        self.is_running = True
        
        # å•Ÿå‹•éŸ³è¨Šè™•ç†ç·šç¨‹
        logger.info("æ­£åœ¨å•Ÿå‹•éŸ³è¨Šè™•ç†ç·šç¨‹...")
        audio_thread = threading.Thread(target=self._audio_processing_loop)
        audio_thread.daemon = True
        audio_thread.start()
        logger.info("éŸ³è¨Šè™•ç†ç·šç¨‹å·²å•Ÿå‹•")
        
        # å•Ÿå‹•è¦–è¦ºåŒ–
        await self._start_visualization()
    
    async def test_audio_file(self, file_path: str):
        """æ¸¬è©¦éŸ³è¨Šæª”æ¡ˆ"""
        logger.info(f"æ¸¬è©¦éŸ³è¨Šæª”æ¡ˆ: {file_path}")
        self.test_mode = 'file'
        
        # TODO: å¯¦ä½œéŸ³è¨Šæª”æ¡ˆæ¸¬è©¦
        pass
    
    async def test_synthetic_audio(self):
        """æ¸¬è©¦åˆæˆéŸ³è¨Šï¼ˆç´”èªéŸ³ã€ç´”éœéŸ³ã€æ··åˆï¼‰"""
        logger.info("é–‹å§‹åˆæˆéŸ³è¨Šæ¸¬è©¦")
        self.test_mode = 'synthetic'
        
        # è¨­å®šè¦–è¦ºåŒ–ï¼ˆå¦‚æœéœ€è¦çš„è©±ï¼‰
        # è¨»ï¼šåˆæˆéŸ³è¨Šæ¸¬è©¦é€šå¸¸ä¸éœ€è¦è¦–è¦ºåŒ–ï¼Œæ‰€ä»¥æˆ‘å€‘è·³éè¦–è¦ºåŒ–è¨­å®š
        
        test_cases = [
            ("ç´”éœéŸ³", self._generate_silence, 3.0),
            ("ç´”èªéŸ³", self._generate_speech, 3.0),
            ("èªéŸ³+éœéŸ³", self._generate_speech_with_silence, 5.0),
            ("å™ªéŸ³ç’°å¢ƒ", self._generate_noisy_speech, 5.0)
        ]
        
        for name, generator, duration in test_cases:
            logger.info(f"\næ¸¬è©¦å ´æ™¯: {name}")
            logger.info("-" * 40)
            
            # é‡ç½®çµ±è¨ˆ
            self.statistics_collector.reset()
            await self.vad_operator.flush()
            
            # ç”Ÿæˆæ¸¬è©¦éŸ³è¨Š
            audio_data = generator(duration)
            
            # è™•ç†éŸ³è¨Š
            start_time = time.time()
            await self._process_audio_data(audio_data)
            processing_time = time.time() - start_time
            
            # è¼¸å‡ºçµæœ
            stats = self.statistics_collector.get_statistics()
            logger.info(f"è™•ç†æ™‚é–“: {processing_time:.3f}s")
            logger.info(f"ç¸½å¹€æ•¸: {stats.total_frames}")
            logger.info(f"èªéŸ³å¹€æ•¸: {stats.speech_frames}")
            logger.info(f"éœéŸ³å¹€æ•¸: {stats.silence_frames}")
            logger.info(f"èªéŸ³æ¯”ä¾‹: {stats.speech_frames / max(1, stats.total_frames):.2%}")
            logger.info(f"èªéŸ³æ®µè½æ•¸: {len(stats.speech_segments)}")
            
            if stats.speech_segments:
                logger.info(f"å¹³å‡æ®µè½æ™‚é•·: {stats.average_segment_duration:.3f}s")
                logger.info(f"æœ€é•·æ®µè½: {stats.max_segment_duration:.3f}s")
            
            await asyncio.sleep(0.5)
    
    def _audio_processing_loop(self):
        """éŸ³è¨Šè™•ç†è¿´åœˆï¼ˆåœ¨ç·šç¨‹ä¸­é‹è¡Œï¼‰"""
        logger.info("éŸ³è¨Šè™•ç†ç·šç¨‹å•Ÿå‹•")
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        
        frame_count = 0
        while self.is_running:
            try:
                # æª¢æŸ¥ stream æ˜¯å¦ä»ç„¶é–‹å•Ÿ
                if not self.stream or not self.stream.is_active():
                    logger.info("éŸ³è¨Šæµå·²é—œé–‰ï¼Œé€€å‡ºè™•ç†å¾ªç’°")
                    break
                    
                # è®€å–éŸ³è¨Š - ä½¿ç”¨æ­£ç¢ºçš„ mic_chunk_size
                audio_data = self.stream.read(self.mic_chunk_size, exception_on_overflow=False)
                frame_count += 1
                
                if frame_count <= 5:  # åªè¨˜éŒ„å‰5å¹€
                    logger.info(f"è®€å–åˆ°ç¬¬ {frame_count} å¹€éŸ³è¨Š: {len(audio_data)} bytes")
                
                # è½‰æ›ç‚º numpy arrayï¼ˆè€ƒæ…®ç«‹é«”è²ï¼‰
                audio_np = np.frombuffer(audio_data, dtype=np.int16).reshape(-1, self.mic_channels)
                
                # è™•ç†éŸ³è¨Š
                try:
                    loop.run_until_complete(self._process_audio_chunk(audio_data, audio_np))
                except Exception as process_error:
                    logger.error(f"è™•ç†éŸ³è¨Šå¡Šæ™‚ç™¼ç”ŸéŒ¯èª¤: {process_error}", exc_info=True)
                    if frame_count <= 5:
                        raise  # å‰5å¹€çš„éŒ¯èª¤éœ€è¦ç«‹å³å ±å‘Š
                
            except Exception as e:
                if self.is_running:  # åªåœ¨ä»åœ¨é‹è¡Œæ™‚å ±éŒ¯
                    logger.error(f"éŸ³è¨Šè™•ç†éŒ¯èª¤: {e}")
                    time.sleep(0.01)
                else:
                    break
        
        loop.close()
    
    async def _process_audio_chunk(self, audio_data: bytes, audio_np: np.ndarray):
        """è™•ç†å–®å€‹éŸ³è¨Šå¡Š"""
        # è¨˜éŒ„è™•ç†é–‹å§‹æ™‚é–“
        start_time = time.time()
        
        # è¨˜éŒ„é–‹å§‹è™•ç†
        chunk_number = getattr(self, '_chunk_count', 0) + 1
        self._chunk_count = chunk_number
        if chunk_number <= 5:
            logger.info(f"é–‹å§‹è™•ç†éŸ³è¨Šå¡Š #{chunk_number}")
        
        # å‰µå»ºéº¥å…‹é¢¨éŸ³è¨Šçš„å…ƒæ•¸æ“š
        mic_metadata = {
            'metadata': AudioMetadata(
                sample_rate=self.mic_sample_rate,
                channels=self.mic_channels,
                format=AudioFormat.INT16
            )
        }
        
        # è½‰æ›éŸ³è¨Šæ ¼å¼ï¼ˆ48kHz ç«‹é«”è² 16bit -> 16kHz å–®è²é“ 16bitï¼‰
        if chunk_number <= 5:
            logger.info(f"é–‹å§‹éŸ³è¨Šæ ¼å¼è½‰æ›... è¼¸å…¥å¤§å°: {len(audio_data)} bytes")
        converted_audio = await self.format_converter.process(audio_data, **mic_metadata)
        if chunk_number <= 5:
            if converted_audio:
                logger.info(f"éŸ³è¨Šæ ¼å¼è½‰æ›å®Œæˆï¼Œè¼¸å‡ºå¤§å°: {len(converted_audio)} bytes")
                # è¨ˆç®—é æœŸå¤§å°
                expected_size = len(audio_data) * 16000 // 48000 // 2  # æ¡æ¨£ç‡é™ä½ + ç«‹é«”è½‰å–®è²é“
                logger.info(f"é æœŸè¼¸å‡ºå¤§å°: {expected_size} bytes")
            else:
                logger.error("éŸ³è¨Šæ ¼å¼è½‰æ›è¿”å› None")
        
        if converted_audio is None:
            logger.error("éŸ³è¨Šæ ¼å¼è½‰æ›å¤±æ•—")
            return
        
        # è½‰æ›å¾Œçš„éŸ³è¨Šç‚º numpy arrayï¼ˆç”¨æ–¼è¦–è¦ºåŒ–ï¼‰
        converted_np = np.frombuffer(converted_audio, dtype=np.int16)
        
        # è¨ˆç®—éŸ³è¨Šèƒ½é‡ï¼ˆç”¨æ–¼èª¿è©¦ï¼‰
        audio_energy = np.abs(converted_np).mean()
        
        # å‰µå»º VAD å…ƒæ•¸æ“š
        vad_metadata = {
            'metadata': self.vad_format
        }
        
        # åŸ·è¡Œ VAD
        if chunk_number <= 5:
            logger.info("é–‹å§‹ VAD è™•ç†...")
        
        # VAD éœ€è¦ kwargs ä½œç‚ºå­—å…¸å‚³éï¼Œä¸è¦è§£åŒ…
        result = await self.vad_operator.process(converted_audio, metadata=self.vad_format)
        
        if chunk_number <= 5:
            logger.info("VAD è™•ç†å®Œæˆ")
        
        # è¨˜éŒ„è™•ç†æ™‚é–“
        processing_time = time.time() - start_time
        self.statistics_collector.add_processing_time(processing_time)
        
        # ç²å– VAD ç‹€æ…‹
        vad_state = self.vad_operator.get_info()
        
        # èª¿è©¦è¼¸å‡º
        if chunk_number <= 5:
            logger.info(f"éŸ³è¨Šè™•ç† #{chunk_number}: åŸå§‹={len(audio_data)} bytes, è½‰æ›å¾Œ={len(converted_audio)} bytes, èƒ½é‡={audio_energy:.4f}, VADç‹€æ…‹={vad_state.get('in_speech', False)}, èªéŸ³æ©Ÿç‡={vad_state.get('speech_probability', 0):.4f}")
        
        # å°‡è³‡æ–™åŠ å…¥è¦–è¦ºåŒ–ä½‡åˆ—ï¼ˆä½¿ç”¨è½‰æ›å¾Œçš„éŸ³è¨Šï¼‰
        self.visualization.add_data({
            'audio': converted_np,
            'vad_state': vad_state,
            'timestamp': time.time(),
            'speech_prob': vad_state.get('speech_probability', 0),
            'threshold': self.vad_operator.threshold
        })
    
    async def _process_audio_data(self, audio_data: np.ndarray):
        """è™•ç†å®Œæ•´çš„éŸ³è¨Šè³‡æ–™"""
        # è½‰æ›ç‚º bytes
        audio_bytes = audio_data.astype(np.int16).tobytes()
        
        # åˆ†å¡Šè™•ç†
        chunk_size_bytes = self.chunk_size * 2  # int16
        
        for i in range(0, len(audio_bytes), chunk_size_bytes):
            chunk = audio_bytes[i:i + chunk_size_bytes]
            if len(chunk) == chunk_size_bytes:
                await self.vad_operator.process(chunk)
    
    async def _on_speech_start(self, event_data: Dict[str, Any]):
        """èªéŸ³é–‹å§‹äº‹ä»¶"""
        logger.info(f"ğŸ¤ èªéŸ³é–‹å§‹ (æ©Ÿç‡: {event_data.get('speech_probability', 0):.3f})")
    
    async def _on_speech_end(self, event_data: Dict[str, Any]):
        """èªéŸ³çµæŸäº‹ä»¶"""
        duration = event_data.get('speech_duration', 0)
        logger.info(f"ğŸ”‡ èªéŸ³çµæŸ (æ™‚é•·: {duration:.3f}s)")
    
    async def _on_vad_result(self, vad_result: Dict[str, Any]):
        """VAD çµæœäº‹ä»¶"""
        # æ”¶é›†çµ±è¨ˆ
        frame = VADFrame(
            timestamp=vad_result['timestamp'],
            speech_probability=vad_result['speech_probability'],
            is_speech=vad_result['speech_detected'],
            threshold=self.vad_operator.threshold
        )
        self.statistics_collector.add_frame(frame)
        
        # è¨˜éŒ„æ™‚é–“æˆ³
        current_time = time.time()
        
        # èª¿è©¦è¼¸å‡º - é¡¯ç¤ºæ‰€æœ‰ VAD çµæœ
        logger.info(f"VAD çµæœ: æ©Ÿç‡={vad_result['speech_probability']:.3f}, æª¢æ¸¬={vad_result['speech_detected']}")
        
        # åªåœ¨å³æ™‚æ¨¡å¼ä¸‹æ›´æ–°è¦–è¦ºåŒ–ï¼ˆç•¶è¦–è¦ºåŒ–å·²ç¶“è¢«è¨­å®šæ™‚ï¼‰
        if self.test_mode == 'realtime' and hasattr(self.visualization, 'lines') and 'vad' in self.visualization.lines:
            self.visualization.update_vad_plot(
                vad_result['speech_probability'], 
                current_time, 
                self.vad_operator.threshold
            )
    
    async def _start_visualization(self):
        """å•Ÿå‹•è¦–è¦ºåŒ–"""
        logger.info("å•Ÿå‹• VAD è¦–è¦ºåŒ–ç›£æ§...")
        
        # è¨­å®šåœ–è¡¨
        self.visualization.setup_plot()
        
        # å•Ÿå‹•å‹•ç•«
        self.visualization.start_animation(self._update_plot, interval=100)
    
    def _update_plot(self, frame):
        """æ›´æ–°åœ–è¡¨"""
        # ç²å–æœ€æ–°æ•¸æ“š
        latest_data = self.visualization.get_latest_data()
        
        if latest_data:
            # æ›´æ–°éŸ³è¨Šæ³¢å½¢
            audio_data = latest_data['audio']
            self.visualization.update_audio_plot(audio_data)
            
            # ä¸éœ€è¦å†æ¬¡æ›´æ–° VAD åœ–è¡¨ï¼Œå› ç‚ºå·²ç¶“åœ¨å›èª¿ä¸­æ›´æ–°äº†
            # åªéœ€è¦ç¢ºä¿åœ–è¡¨é‡ç¹ª
            
            # ç²å–ç•¶å‰é–¾å€¼
            threshold = latest_data.get('threshold', self.vad_operator.threshold)
            
            # æ›´æ–°çµ±è¨ˆ
            stats = self.statistics_collector.get_statistics()
            recent_stats = self.statistics_collector.get_recent_statistics(window_seconds=10)
            
            # ç²å– VAD ç‹€æ…‹
            vad_state = latest_data.get('vad_state', {})
            
            # ä½¿ç”¨ç°¡æ½”çš„æ ¼å¼
            speech_ratio = stats.speech_frames / max(1, stats.total_frames)
            is_speaking = '[èªªè©±ä¸­]' if vad_state.get('is_speaking', False) else '[éœéŸ³]'
            
            # è¨ˆç®—ç´¯ç©æ™‚é•·
            total_duration = time.time() - self.statistics_collector.start_time if hasattr(self.statistics_collector, 'start_time') else 0
            speech_duration = stats.total_speech_duration
            silence_duration = total_duration - speech_duration
            
            stats_text = (
                f"è™•ç†: {stats.total_frames} å¹€ | èªéŸ³: {stats.speech_frames} ({speech_ratio:.1%}) | "
                f"æœ€è¿‘10ç§’: {recent_stats.get('speech_ratio', 0):.1%}\n"
                f"{is_speaking} | èªéŸ³: {self.visualization.format_time(speech_duration)} | "
                f"éœéŸ³: {self.visualization.format_time(silence_duration)} | "
                f"é–¾å€¼: {threshold:.3f}"
            )
            self.visualization.texts['stats'].set_text(stats_text)
        
        return []
    
    def _generate_silence(self, duration: float) -> np.ndarray:
        """ç”Ÿæˆç´”éœéŸ³"""
        samples = int(duration * self.sample_rate)
        return np.zeros(samples)
    
    def _generate_speech(self, duration: float) -> np.ndarray:
        """ç”Ÿæˆæ¨¡æ“¬èªéŸ³ï¼ˆä½¿ç”¨æ›´å¼·çš„å™ªéŸ³ä¿¡è™Ÿï¼‰"""
        samples = int(duration * self.sample_rate)
        t = np.linspace(0, duration, samples)
        
        # ä½¿ç”¨æ›´æ¥è¿‘çœŸå¯¦èªéŸ³çš„å™ªéŸ³æ¨¡å¼
        # ç”¢ç”Ÿå¼·çƒˆçš„å¯¬é »å™ªéŸ³ï¼ˆé¡ä¼¼èªéŸ³é »è­œï¼‰
        speech = np.random.normal(0, 5000, samples)
        
        # åŠ å…¥ä¸€äº›é€±æœŸæ€§å…ƒç´ 
        for freq in [100, 200, 300, 400]:
            speech += 1000 * np.sin(2 * np.pi * freq * t)
        
        # ä½¿ç”¨æ›´æ˜é¡¯çš„æŒ¯å¹…èª¿è£½
        envelope = 0.5 + 0.5 * np.sin(2 * np.pi * 2 * t)
        envelope = np.maximum(envelope, 0.3)  # ä¿æŒæœ€ä½æŒ¯å¹…
        speech *= envelope
        
        # åŠ å…¥æ›´å¤šè®ŠåŒ–çš„å™ªéŸ³
        noise_envelope = np.random.uniform(0.8, 1.2, samples)
        speech *= noise_envelope
        
        # æ­£è¦åŒ–åˆ°æ¥è¿‘æ»¿åˆ»åº¦
        speech = np.clip(speech, -32000, 32000)
        
        return speech
    
    def _generate_speech_with_silence(self, duration: float) -> np.ndarray:
        """ç”ŸæˆèªéŸ³å’ŒéœéŸ³äº¤æ›¿çš„éŸ³è¨Š"""
        samples = int(duration * self.sample_rate)
        audio = np.zeros(samples)
        
        # äº¤æ›¿çš„èªéŸ³å’ŒéœéŸ³æ®µ
        segments = [
            (0.0, 0.5, 'silence'),
            (0.5, 2.0, 'speech'),
            (2.0, 2.5, 'silence'),
            (2.5, 4.0, 'speech'),
            (4.0, 5.0, 'silence')
        ]
        
        for start, end, type_ in segments:
            start_idx = int(start * self.sample_rate)
            end_idx = int(end * self.sample_rate)
            
            if type_ == 'speech':
                segment_duration = end - start
                speech = self._generate_speech(segment_duration)
                audio[start_idx:end_idx] = speech[:end_idx - start_idx]
        
        return audio
    
    def _generate_noisy_speech(self, duration: float) -> np.ndarray:
        """ç”Ÿæˆå¸¶å™ªéŸ³çš„èªéŸ³"""
        # ç”ŸæˆèªéŸ³
        speech = self._generate_speech_with_silence(duration)
        
        # æ·»åŠ ç™½å™ªéŸ³
        noise = np.random.normal(0, 1000, len(speech))
        
        # æ··åˆï¼ˆSNR ç´„ 10dBï¼‰
        noisy_speech = speech + noise
        
        # é™åˆ¶ç¯„åœ
        noisy_speech = np.clip(noisy_speech, -32768, 32767)
        
        return noisy_speech
    
    def print_test_results(self):
        """æ‰“å°æ¸¬è©¦çµæœ"""
        stats = self.statistics_collector.get_statistics()
        
        print("\n" + "="*60)
        print("ğŸ“Š VAD æ¸¬è©¦çµæœ")
        print("="*60)
        
        print(f"\nåŸºæœ¬çµ±è¨ˆ:")
        print(f"  ç¸½è™•ç†å¹€æ•¸: {stats.total_frames}")
        print(f"  èªéŸ³å¹€æ•¸: {stats.speech_frames}")
        print(f"  éœéŸ³å¹€æ•¸: {stats.silence_frames}")
        print(f"  èªéŸ³æ¯”ä¾‹: {stats.speech_frames / max(1, stats.total_frames):.2%}")
        
        print(f"\nèªéŸ³æ®µè½:")
        print(f"  æ®µè½æ•¸é‡: {len(stats.speech_segments)}")
        if stats.speech_segments:
            print(f"  ç¸½èªéŸ³æ™‚é•·: {stats.total_speech_duration:.3f}s")
            print(f"  å¹³å‡æ®µè½æ™‚é•·: {stats.average_segment_duration:.3f}s")
            print(f"  æœ€é•·æ®µè½: {stats.max_segment_duration:.3f}s")
            print(f"  æœ€çŸ­æ®µè½: {stats.min_segment_duration:.3f}s")
        
        print(f"\nè™•ç†æ•ˆèƒ½:")
        if stats.processing_times:
            print(f"  å¹³å‡è™•ç†æ™‚é–“: {stats.avg_processing_time * 1000:.3f}ms")
            print(f"  æœ€å¤§è™•ç†æ™‚é–“: {stats.max_processing_time * 1000:.3f}ms")
        
        print("="*60)


async def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸ¯ VAD æ•´åˆæ¸¬è©¦å·¥å…·")
    print("è«‹é¸æ“‡æ¸¬è©¦æ¨¡å¼:")
    print("1. å³æ™‚éŸ³è¨Šæ¸¬è©¦")
    print("2. éŸ³è¨Šæª”æ¡ˆæ¸¬è©¦")
    print("3. åˆæˆéŸ³è¨Šæ¸¬è©¦")
    
    choice = input("\nè«‹è¼¸å…¥é¸æ“‡ (1-3): ").strip()
    
    tester = VADIntegrationTester()
    
    try:
        # è¨­å®šæ¸¬è©¦ç’°å¢ƒ
        await tester.setup()
        
        if choice == "1":
            print("\né–‹å§‹å³æ™‚éŸ³è¨Šæ¸¬è©¦...")
            print("è«‹å°è‘—éº¥å…‹é¢¨èªªè©±ï¼ŒæŒ‰ Ctrl+C çµæŸæ¸¬è©¦")
            await tester.test_realtime()
            
        elif choice == "2":
            file_path = input("è«‹è¼¸å…¥éŸ³è¨Šæª”æ¡ˆè·¯å¾‘: ").strip()
            await tester.test_audio_file(file_path)
            
        elif choice == "3":
            print("\né–‹å§‹åˆæˆéŸ³è¨Šæ¸¬è©¦...")
            await tester.test_synthetic_audio()
            
        else:
            print("ç„¡æ•ˆçš„é¸æ“‡")
            return
        
    except KeyboardInterrupt:
        print("\næ¸¬è©¦è¢«ç”¨æˆ¶ä¸­æ–·")
    except Exception as e:
        print(f"\næ¸¬è©¦éŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()
    finally:
        # æ¸…ç†è³‡æº
        await tester.cleanup()
        
        # æ‰“å°çµæœ
        tester.print_test_results()


if __name__ == "__main__":
    asyncio.run(main())