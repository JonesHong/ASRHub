#!/usr/bin/env python3
"""
å–šé†’è©æ•ˆèƒ½åŸºæº–æ¸¬è©¦å·¥å…·
æ¸¬è©¦ä¸åŒé…ç½®ä¸‹çš„åµæ¸¬æº–ç¢ºç‡å’Œæ•ˆèƒ½
"""

import asyncio
import os
import sys
import time
import json
import argparse
from datetime import datetime
from typing import Dict, Any, List, Optional
import numpy as np
import statistics

# æ·»åŠ  src åˆ°è·¯å¾‘
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'src'))

from src.operators.wakeword import OpenWakeWordOperator
from src.utils.logger import logger
from src.config.manager import ConfigManager


class WakeWordBenchmark:
    """å–šé†’è©åŸºæº–æ¸¬è©¦å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–åŸºæº–æ¸¬è©¦å™¨"""
        self.config_manager = ConfigManager()
        
        # å¾é…ç½®è®€å–éŸ³è¨Šåƒæ•¸        
        # æ¸¬è©¦é…ç½®
        self.test_configs = [
            {"threshold": 0.3, "name": "ä½é–¾å€¼ (0.3)"},
            {"threshold": 0.5, "name": "ä¸­ç­‰é–¾å€¼ (0.5)"},
            {"threshold": 0.7, "name": "é«˜é–¾å€¼ (0.7)"},
        ]
        
        # æ¸¬è©¦çµæœ
        self.results: List[Dict[str, Any]] = []
        
        # æ¨¡æ“¬éŸ³è¨Šè³‡æ–™ï¼ˆç”¨æ–¼æ•ˆèƒ½æ¸¬è©¦ï¼‰
        self.sample_rate = self.config_manager.pipeline.default_sample_rate
        self.chunk_size = 1280
        self.test_duration = 30  # ç§’
    
    def generate_test_audio(self, duration: float) -> bytes:
        """
        ç”Ÿæˆæ¸¬è©¦éŸ³è¨Šè³‡æ–™
        
        Args:
            duration: éŸ³è¨Šé•·åº¦ï¼ˆç§’ï¼‰
            
        Returns:
            éŸ³è¨Šè³‡æ–™ bytes
        """
        samples = int(duration * self.sample_rate)
        
        # ç”Ÿæˆç™½å™ªéŸ³
        audio_data = np.random.normal(0, 100, samples).astype(np.int16)
        
        return audio_data.tobytes()
    
    async def benchmark_config(self, config: Dict[str, Any]) -> Dict[str, Any]:
        """
        æ¸¬è©¦å–®å€‹é…ç½®
        
        Args:
            config: æ¸¬è©¦é…ç½®
            
        Returns:
            æ¸¬è©¦çµæœ
        """
        print(f"\nğŸ§ª æ¸¬è©¦é…ç½®: {config['name']}")
        print("-" * 40)
        
        # åˆå§‹åŒ–æ“ä½œå™¨
        operator = OpenWakeWordOperator()
        # æ›´æ–°é–¾å€¼è¨­å®š
        operator.update_config({"threshold": config["threshold"]})
        detections = []
        processing_times = []
        
        # åµæ¸¬å›å‘¼
        async def on_detection(detection):
            detections.append(detection)
        
        operator.set_detection_callback(on_detection)
        
        try:
            # å•Ÿå‹•æ“ä½œå™¨
            await operator.start()
            print("âœ… æ“ä½œå™¨å·²å•Ÿå‹•")
            
            # ç”Ÿæˆæ¸¬è©¦éŸ³è¨Š
            test_audio = self.generate_test_audio(self.test_duration)
            total_chunks = len(test_audio) // (self.chunk_size * 2)  # int16 = 2 bytes
            
            print(f"ğŸµ æ¸¬è©¦éŸ³è¨Š: {self.test_duration} ç§’")
            print(f"ğŸ“¦ éŸ³è¨Šå¡Šæ•¸: {total_chunks}")
            
            # é–‹å§‹æ•ˆèƒ½æ¸¬è©¦
            start_time = time.time()
            processed_chunks = 0
            
            for i in range(0, len(test_audio), self.chunk_size * 2):
                chunk = test_audio[i:i + self.chunk_size * 2]
                if len(chunk) < self.chunk_size * 2:
                    break
                
                # æ¸¬é‡è™•ç†æ™‚é–“
                chunk_start = time.time()
                
                await operator.process(
                    chunk,
                    sample_rate=self.sample_rate,
                    session_id="benchmark"
                )
                
                chunk_end = time.time()
                processing_times.append(chunk_end - chunk_start)
                processed_chunks += 1
                
                # é¡¯ç¤ºé€²åº¦
                if processed_chunks % 100 == 0:
                    progress = processed_chunks / total_chunks * 100
                    print(f"â³ é€²åº¦: {progress:.1f}%", end='\r')
            
            end_time = time.time()
            total_time = end_time - start_time
            
            print(f"\nâœ… æ¸¬è©¦å®Œæˆ")
            
            # è¨ˆç®—çµ±è¨ˆè³‡è¨Š
            avg_processing_time = statistics.mean(processing_times) if processing_times else 0
            max_processing_time = max(processing_times) if processing_times else 0
            min_processing_time = min(processing_times) if processing_times else 0
            
            throughput = processed_chunks / total_time if total_time > 0 else 0
            real_time_factor = (processed_chunks * self.chunk_size / self.sample_rate) / total_time if total_time > 0 else 0
            
            # åˆ†æåµæ¸¬çµæœ
            detection_count = len(detections)
            detection_scores = [d.get("score", 0) for d in detections]
            avg_score = statistics.mean(detection_scores) if detection_scores else 0
            max_score = max(detection_scores) if detection_scores else 0
            
            result = {
                "config": config,
                "test_duration": self.test_duration,
                "processed_chunks": processed_chunks,
                "total_processing_time": total_time,
                "avg_chunk_processing_time": avg_processing_time,
                "max_chunk_processing_time": max_processing_time,
                "min_chunk_processing_time": min_processing_time,
                "throughput_chunks_per_sec": throughput,
                "real_time_factor": real_time_factor,
                "detection_count": detection_count,
                "avg_detection_score": avg_score,
                "max_detection_score": max_score,
                "timestamp": datetime.now().isoformat()
            }
            
            # æ‰“å°çµæœ
            self._print_config_results(result)
            
            return result
            
        except Exception as e:
            logger.error(f"é…ç½®æ¸¬è©¦å¤±æ•—: {e}")
            return {
                "config": config,
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }
        
        finally:
            # æ¸…ç†
            await operator.stop()
    
    def _print_config_results(self, result: Dict[str, Any]):
        """æ‰“å°é…ç½®æ¸¬è©¦çµæœ"""
        print(f"\nğŸ“Š {result['config']['name']} æ¸¬è©¦çµæœ:")
        print(f"â±ï¸  ç¸½è™•ç†æ™‚é–“: {result['total_processing_time']:.2f} ç§’")
        print(f"ğŸ“¦ è™•ç†å¡Šæ•¸: {result['processed_chunks']}")
        print(f"âš¡ ååé‡: {result['throughput_chunks_per_sec']:.1f} å¡Š/ç§’")
        print(f"ğŸ•’ å¯¦æ™‚å€æ•¸: {result['real_time_factor']:.2f}x")
        print(f"ğŸ“ˆ å¹³å‡å¡Šè™•ç†æ™‚é–“: {result['avg_chunk_processing_time']*1000:.2f} ms")
        print(f"ğŸ“Š æœ€å¤§å¡Šè™•ç†æ™‚é–“: {result['max_chunk_processing_time']*1000:.2f} ms")
        print(f"ğŸ“‰ æœ€å°å¡Šè™•ç†æ™‚é–“: {result['min_chunk_processing_time']*1000:.2f} ms")
        print(f"ğŸ¯ åµæ¸¬æ¬¡æ•¸: {result['detection_count']}")
        print(f"ğŸ”¢ å¹³å‡åµæ¸¬åˆ†æ•¸: {result['avg_detection_score']:.3f}")
        print(f"ğŸ“ˆ æœ€é«˜åµæ¸¬åˆ†æ•¸: {result['max_detection_score']:.3f}")
    
    async def run_benchmark(self, output_file: Optional[str] = None):
        """
        åŸ·è¡Œå®Œæ•´çš„åŸºæº–æ¸¬è©¦
        
        Args:
            output_file: çµæœè¼¸å‡ºæª”æ¡ˆè·¯å¾‘
        """
        print("ğŸš€ é–‹å§‹å–šé†’è©æ•ˆèƒ½åŸºæº–æ¸¬è©¦")
        print("=" * 50)
        
        start_time = datetime.now()
        
        # æ¸¬è©¦æ¯å€‹é…ç½®
        for config in self.test_configs:
            result = await self.benchmark_config(config)
            self.results.append(result)
        
        end_time = datetime.now()
        total_time = (end_time - start_time).total_seconds()
        
        # æ‰“å°ç¸½çµ
        self._print_summary(total_time)
        
        # ä¿å­˜çµæœ
        if output_file:
            self._save_results(output_file)
    
    def _print_summary(self, total_time: float):
        """æ‰“å°æ¸¬è©¦ç¸½çµ"""
        print("\n" + "=" * 50)
        print("ğŸ“ˆ åŸºæº–æ¸¬è©¦ç¸½çµ")
        print("=" * 50)
        print(f"â±ï¸  ç¸½æ¸¬è©¦æ™‚é–“: {total_time:.1f} ç§’")
        print(f"ğŸ§ª æ¸¬è©¦é…ç½®æ•¸: {len(self.test_configs)}")
        print(f"ğŸ“Š æœ‰æ•ˆçµæœæ•¸: {len([r for r in self.results if 'error' not in r])}")
        
        # æ‰¾å‡ºæœ€ä½³é…ç½®
        valid_results = [r for r in self.results if 'error' not in r]
        
        if valid_results:
            # æŒ‰å¯¦æ™‚å€æ•¸æ’åºï¼ˆè¶Šé«˜è¶Šå¥½ï¼‰
            best_performance = max(valid_results, key=lambda x: x['real_time_factor'])
            
            # æŒ‰åµæ¸¬åˆ†æ•¸æ’åºï¼ˆéœ€è¦å¹³è¡¡åµæ¸¬æ¬¡æ•¸å’Œåˆ†æ•¸ï¼‰
            best_detection = max(valid_results, key=lambda x: x['avg_detection_score'])
            
            print(f"\nğŸ† æœ€ä½³æ•ˆèƒ½é…ç½®: {best_performance['config']['name']}")
            print(f"   å¯¦æ™‚å€æ•¸: {best_performance['real_time_factor']:.2f}x")
            print(f"   ååé‡: {best_performance['throughput_chunks_per_sec']:.1f} å¡Š/ç§’")
            
            print(f"\nğŸ¯ æœ€ä½³åµæ¸¬é…ç½®: {best_detection['config']['name']}")
            print(f"   å¹³å‡åˆ†æ•¸: {best_detection['avg_detection_score']:.3f}")
            print(f"   åµæ¸¬æ¬¡æ•¸: {best_detection['detection_count']}")
        
        print("=" * 50)
    
    def _save_results(self, output_file: str):
        """ä¿å­˜æ¸¬è©¦çµæœåˆ°æª”æ¡ˆ"""
        try:
            # æº–å‚™è¼¸å‡ºè³‡æ–™
            output_data = {
                "benchmark_info": {
                    "test_duration": self.test_duration,
                    "sample_rate": self.sample_rate,
                    "chunk_size": self.chunk_size,
                    "timestamp": datetime.now().isoformat()
                },
                "results": self.results
            }
            
            # å¯«å…¥æª”æ¡ˆ
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(output_data, f, indent=2, ensure_ascii=False)
            
            print(f"ğŸ’¾ çµæœå·²ä¿å­˜åˆ°: {output_file}")
            
        except Exception as e:
            logger.error(f"ä¿å­˜çµæœå¤±æ•—: {e}")


def main():
    """ä¸»å‡½æ•¸"""
    parser = argparse.ArgumentParser(description="å–šé†’è©æ•ˆèƒ½åŸºæº–æ¸¬è©¦")
    parser.add_argument(
        "--output", 
        "-o", 
        type=str, 
        help="çµæœè¼¸å‡ºæª”æ¡ˆè·¯å¾‘ (JSONæ ¼å¼)"
    )
    parser.add_argument(
        "--duration", 
        "-d", 
        type=int, 
        default=30, 
        help="æ¸¬è©¦éŸ³è¨Šé•·åº¦ï¼ˆç§’ï¼Œé è¨­: 30ï¼‰"
    )
    
    args = parser.parse_args()
    
    print("âš¡ ASR Hub å–šé†’è©æ•ˆèƒ½åŸºæº–æ¸¬è©¦å·¥å…·")
    print("=" * 50)
    
    # æª¢æŸ¥ç’°å¢ƒ
    if not os.environ.get("HF_TOKEN"):
        print("âš ï¸  è­¦å‘Š: æœªè¨­å®š HF_TOKEN ç’°å¢ƒè®Šæ•¸")
        print("å¦‚æœéœ€è¦ä¸‹è¼‰æ¨¡å‹ï¼Œè«‹è¨­å®š: export HF_TOKEN=your_token")
        print()
    
    benchmark = WakeWordBenchmark()
    benchmark.test_duration = args.duration
    
    try:
        asyncio.run(benchmark.run_benchmark(args.output))
    except KeyboardInterrupt:
        print("\nâŒ æ¸¬è©¦è¢«ç”¨æˆ¶ä¸­æ–·")
    except Exception as e:
        print(f"\nâŒ æ¸¬è©¦éŒ¯èª¤: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()