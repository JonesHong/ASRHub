#!/usr/bin/env python3
"""
openWakeWord è¦–è¦ºåŒ–è¨ºæ–·ç¨‹å¼ï¼ˆåŸºæ–¼ test_kmu.py çš„è™•ç†é‚è¼¯ï¼‰
é¡¯ç¤ºå³æ™‚åˆ†æ•¸å’ŒéŸ³é »æ³¢å½¢ï¼Œå¹«åŠ©è¨ºæ–·æª¢æ¸¬å•é¡Œ
"""

import os
import numpy as np
import pyaudio
import collections
from functools import partial
import time
from datetime import datetime
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation
from huggingface_hub import hf_hub_download, HfFolder
from openwakeword.model import Model
from openwakeword.utils import download_models
import queue
import threading
import scipy.signal

# è¨­å®šä¸­æ–‡å­—é«”
plt.rcParams["font.sans-serif"] = ["Arial Unicode MS", "PingFang SC", "SimHei", "DejaVu Sans"]
plt.rcParams["axes.unicode_minus"] = False


class WakeWordVisualizer:
    def __init__(self):
        self.chunk_size = 1280
        self.sample_rate = 16000
        self.channels = 1
        self.format = pyaudio.paInt16  # ä½¿ç”¨ Int16 æ ¼å¼
        
        # ä½¿ç”¨èˆ‡ test_kmu.py ç›¸åŒçš„ç‹€æ…‹ç®¡ç†
        self.state = collections.defaultdict(partial(collections.deque, maxlen=60))
        
        # æ•¸æ“šå„²å­˜
        self.audio_queue = queue.Queue()
        self.audio_history = collections.deque(maxlen=16000)  # 1ç§’çš„éŸ³é »
        self.detection_markers = []  # å„²å­˜æª¢æ¸¬æ¨™è¨˜çš„ä½ç½®
        
        # åˆå§‹åŒ–æ¨¡å‹
        self.model = self._init_model()
        
        # åˆå§‹åŒ–éŸ³é »
        self.p = pyaudio.PyAudio()
        self.stream = None
        self.is_running = False
        
        # å„²å­˜å¯¦éš›çš„æ¡æ¨£ç‡ï¼ˆå¯èƒ½ä¸æ˜¯ 16kHzï¼‰
        self.actual_sample_rate = None

    def _init_model(self):
        """åˆå§‹åŒ–æ¨¡å‹ï¼ˆåªè¼‰å…¥ KMU æ¨¡å‹ï¼‰"""
        print("=== åˆå§‹åŒ–æ¨¡å‹ ===")
        download_models()
        
        # åªä½¿ç”¨ KMU æ¨¡å‹ï¼Œèˆ‡ test_kmu.py ç›¸åŒ
        hf_token = os.environ.get("HF_TOKEN") or HfFolder.get_token()
        
        if hf_token:
            print("è¼‰å…¥ KMU æ¨¡å‹...")
            try:
                model_path = hf_hub_download(
                    repo_id="JTBTechnology/kmu_wakeword",
                    filename="hi_kmu_0721.onnx",
                    token=hf_token,
                    repo_type="model"
                )
                # ç›´æ¥ç”¨ä¸‹è¼‰çš„æ¨¡å‹è·¯å¾‘è¼‰å…¥
                model = Model(wakeword_models=[model_path], inference_framework="onnx")
                print(f"âœ“ æ¨¡å‹è¼‰å…¥æˆåŠŸ")
                return model
            except Exception as e:
                print(f"âœ— æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
                return None
        else:
            print("âœ— æœªæ‰¾åˆ° HF_TOKEN")
            return None

    def process_audio_chunk(self, audio_data):
        """è™•ç†éŸ³é »å¡Šï¼ˆåŸºæ–¼ test_kmu.py çš„é‚è¼¯ï¼‰"""
        # æ¨¡æ“¬ Gradio çš„ (sample_rate, audio_data) æ ¼å¼
        audio = (self.actual_sample_rate, audio_data)
        
        # åªæ‰“å°ä¸€æ¬¡èª¿è©¦ä¿¡æ¯
        if not hasattr(self, '_process_debug_printed'):
            self._process_debug_printed = False
        
        # é‡æ¡æ¨£åˆ° 16kHzï¼ˆå¦‚æœéœ€è¦ï¼‰
        if audio[0] != 16000:
            data = scipy.signal.resample(audio[1], int(float(audio[1].shape[0])/audio[0]*16000))
            if not self._process_debug_printed:
                print(f"\n=== é‡æ¡æ¨£è™•ç† ===")
                print(f"åŸå§‹æ¡æ¨£ç‡: {audio[0]} Hz")
                print(f"ç›®æ¨™æ¡æ¨£ç‡: 16000 Hz")
                print(f"åŸå§‹æ¨£æœ¬æ•¸: {audio[1].shape[0]}")
                print(f"é‡æ¡æ¨£å¾Œæ¨£æœ¬æ•¸: {data.shape[0]}")
                self._process_debug_printed = True
        else:
            data = audio[1]
        
        # è™•ç†é æ¸¬ï¼ˆèˆ‡ test_kmu.py ç›¸åŒçš„é‚è¼¯ï¼‰
        predictions = []
        for i in range(0, data.shape[0], 1280):
            # è™•ç†è²é“ï¼ˆé›–ç„¶æˆ‘å€‘æ˜¯å–®è²é“ï¼Œä½†ä¿æŒç›¸åŒé‚è¼¯ï¼‰
            if len(data.shape) == 2 or data.shape[-1] == 2:
                chunk = data[i:i+1280][:, 0]  # åªå–ä¸€å€‹è²é“
            else:
                chunk = data[i:i+1280]
            
            if chunk.shape[0] == 1280:
                prediction = self.model.predict(chunk)
                for key in prediction:
                    # å¦‚æœ deque æ˜¯ç©ºçš„ï¼Œå¡«å……é›¶
                    if len(self.state[key]) == 0:
                        self.state[key].extend(np.zeros(60))
                    
                    # æ·»åŠ é æ¸¬
                    self.state[key].append(prediction[key])
                predictions.append(prediction)
        
        return predictions

    def audio_callback(self):
        """éŸ³é »è™•ç†ç·šç¨‹"""
        # å˜—è©¦ä¸åŒçš„æ¡æ¨£ç‡
        for test_rate in [16000, 44100, 48000]:
            try:
                self.stream = self.p.open(
                    format=self.format,
                    channels=self.channels,
                    rate=test_rate,
                    input=True,
                    frames_per_buffer=self.chunk_size,
                )
                self.actual_sample_rate = test_rate
                print(f"ä½¿ç”¨æ¡æ¨£ç‡: {test_rate} Hz")
                break
            except Exception as e:
                print(f"ç„¡æ³•ä½¿ç”¨æ¡æ¨£ç‡ {test_rate}: {e}")
                continue
        
        if not self.stream:
            print("ç„¡æ³•é–‹å•ŸéŸ³é »æµ")
            return
        
        print("\né–‹å§‹éŒ„éŸ³...")
        print("è«‹èªªå‡ºå–šé†’è©ï¼š'å—¨ï¼Œé«˜é†«' æˆ– 'hi kmu'\n")
        
        # èª¿è©¦æ¨™èªŒï¼ˆåªæ‰“å°ä¸€æ¬¡ï¼‰
        debug_printed = False
        
        while self.is_running:
            try:
                # è®€å–éŸ³é »
                audio_data = self.stream.read(self.chunk_size, exception_on_overflow=False)
                
                # èª¿è©¦ï¼šæ‰“å° PyAudio åŸå§‹éŸ³é »å±¬æ€§ï¼ˆåªæ‰“å°ä¸€æ¬¡ï¼‰
                if not debug_printed:
                    print(f"\n=== PyAudio åŸå§‹éŸ³é »å±¬æ€§ [{datetime.now().strftime('%H:%M:%S.%f')[:-3]}] ===")
                    print(f"audio_data é¡å‹: {type(audio_data)}")
                    print(f"audio_data é•·åº¦: {len(audio_data)} bytes")
                    print(f"é æœŸæ¨£æœ¬æ•¸: {self.chunk_size}")
                    print(f"æ ¼å¼: {self.format} (pyaudio.paInt16)")
                    print(f"å¯¦éš›æ¡æ¨£ç‡: {self.actual_sample_rate} Hz")
                    
                    # è½‰æ›å‰çš„ Int16 æ•¸æ“š
                    raw_int16 = np.frombuffer(audio_data, dtype=np.int16)
                    print(f"\nè½‰æ›å‰ (Int16):")
                    print(f"  æ•¸æ“šé¡å‹: {raw_int16.dtype}")
                    print(f"  æ•¸æ“šå½¢ç‹€: {raw_int16.shape}")
                    print(f"  æ•¸æ“šç¯„åœ: [{raw_int16.min()}, {raw_int16.max()}]")
                    print(f"  å‰5å€‹æ¨£æœ¬: {raw_int16[:5]}")
                
                # è½‰æ›ç‚º float32 
                audio_np = np.frombuffer(audio_data, dtype=np.int16)
                
                # èª¿è©¦ï¼šæª¢æŸ¥éŸ³é »çš„å¯¦éš›ç¯„åœ
                if not debug_printed:
                    actual_max = np.abs(audio_np).max()
                    print(f"\néŸ³é »ç¯„åœåˆ†æ:")
                    print(f"  æœ€å¤§çµ•å°å€¼: {actual_max}")
                    print(f"  æ˜¯å¦éœ€è¦æ­£è¦åŒ–: {actual_max > 1.0}")
                
                # è½‰æ›ç‚º float32 ä½†ä¸æ­£è¦åŒ–ï¼ˆåŒ¹é… Gradio çš„è¡Œç‚ºï¼‰
                # Gradio ä¼¼ä¹ç›´æ¥ä½¿ç”¨ int16 å€¼ä½œç‚º float
                audio_np = audio_np.astype(np.float32)
                
                # èª¿è©¦ï¼šæ‰“å°è½‰æ›å¾Œçš„éŸ³é »å±¬æ€§ï¼ˆåªæ‰“å°ä¸€æ¬¡ï¼‰
                if not debug_printed:
                    print(f"\nè½‰æ›å¾Œ (Float32 - ä¸æ­£è¦åŒ–):")
                    print(f"  æ•¸æ“šé¡å‹: {audio_np.dtype}")
                    print(f"  æ•¸æ“šå½¢ç‹€: {audio_np.shape}")
                    print(f"  æ•¸æ“šç¯„åœ: [{audio_np.min():.3f}, {audio_np.max():.3f}]")
                    print(f"  å‰5å€‹æ¨£æœ¬: {audio_np[:5]}")
                    
                    # æª¢æŸ¥æ˜¯å¦éœ€è¦æ­£è¦åŒ–
                    if np.abs(audio_np).max() > 10:
                        print(f"  æ³¨æ„ï¼šä¿æŒåŸå§‹ç¯„åœï¼Œé¡ä¼¼ Gradio çš„è¡Œç‚º")
                    print("=" * 60)
                    debug_printed = True
                
                # å„²å­˜éŸ³é »æ­·å²
                self.audio_history.extend(audio_np)
                
                # è™•ç†éŸ³é »ï¼ˆä½¿ç”¨ test_kmu.py çš„é‚è¼¯ï¼‰
                predictions = self.process_audio_chunk(audio_np)
                
                # å„²å­˜æ•¸æ“šä¾›è¦–è¦ºåŒ–ä½¿ç”¨
                if self.state:
                    # ç²å–æœ€æ–°çš„åˆ†æ•¸
                    model_name = list(self.state.keys())[0]
                    if len(self.state[model_name]) > 0:
                        score = self.state[model_name][-1]
                        self.audio_queue.put((audio_np, score, model_name))
                        
                        # å¦‚æœåˆ†æ•¸é«˜ï¼Œæ‰“å°è¨Šæ¯
                        current_time = time.time()
                        
                        if score > 0.5:
                            # æª¢æŸ¥æ˜¯å¦åœ¨å†·å»æœŸå…§
                            if current_time - self.last_detection_time > self.detection_cooldown:
                                timestamp = datetime.now().strftime("%H:%M:%S.%f")[:-3]
                                print(f"[{timestamp}] ğŸ¯ æª¢æ¸¬åˆ°å–šé†’è©ï¼åˆ†æ•¸: {score:.3f}")
                                self.last_detection_time = current_time
                                # è¨˜éŒ„æª¢æ¸¬ä½ç½®ï¼ˆç”¨æ–¼åœ–è¡¨æ¨™è¨˜ï¼‰
                                if len(self.state[model_name]) > 0:
                                    self.detection_markers.append(len(self.state[model_name]) - 1)
                        elif score > 0.3:  # ä½åˆ†æ•¸æ´»å‹•ï¼ˆåƒ…ç”¨æ–¼èª¿è©¦ï¼‰
                            if not hasattr(self, '_low_score_logged'):
                                self._low_score_logged = True
                                timestamp = datetime.now().strftime("%H:%M:%S.%f")[:-3]
                                print(f"[{timestamp}] æª¢æ¸¬åˆ°ä½åˆ†æ´»å‹•: {score:.3f}")
                
            except Exception as e:
                print(f"éŸ³é »è™•ç†éŒ¯èª¤: {e}")
        
        self.stream.close()

    def update_plot(self, frame):
        """æ›´æ–°åœ–è¡¨"""
        # æ¸…ç©ºä½‡åˆ—ä¸¦æ›´æ–°æ•¸æ“š
        latest_score = None
        while not self.audio_queue.empty():
            try:
                audio_chunk, score, model_name = self.audio_queue.get_nowait()
                latest_score = score
            except queue.Empty:
                break
        
        # æ›´æ–°åˆ†æ•¸åœ–ï¼ˆä½¿ç”¨ state ä¸­çš„å®Œæ•´æ­·å²ï¼‰
        if self.state:
            model_name = list(self.state.keys())[0]
            scores = list(self.state[model_name])
            if len(scores) > 0:
                self.score_line.set_data(range(len(scores)), scores)
                self.ax1.set_xlim(0, max(60, len(scores)))
                
                # å‹•æ…‹èª¿æ•´ y è»¸ç¯„åœ
                max_score = max(scores) if scores else 1.0
                self.ax1.set_ylim(0, max(1.0, max_score * 1.1))
        
        # æ›´æ–°éŸ³é »æ³¢å½¢
        if len(self.audio_history) > 0:
            audio_data = list(self.audio_history)[-self.sample_rate:]  # æœ€å¾Œ1ç§’
            self.audio_line.set_data(range(len(audio_data)), audio_data)
            self.ax2.set_xlim(0, len(audio_data))
        
        return self.score_line, self.audio_line

    def run(self):
        """åŸ·è¡Œè¦–è¦ºåŒ–"""
        if not self.model:
            print("æ¨¡å‹è¼‰å…¥å¤±æ•—")
            return
        
        # è¨­ç½®åœ–è¡¨
        plt.style.use("dark_background")
        self.fig, (self.ax1, self.ax2) = plt.subplots(2, 1, figsize=(10, 8))
        
        # åˆ†æ•¸åœ–
        self.ax1.set_xlabel("æ™‚é–“ (å¹€)")
        self.ax1.set_ylabel("æª¢æ¸¬åˆ†æ•¸")
        self.ax1.set_title("å–šé†’è©æª¢æ¸¬åˆ†æ•¸ - hi_kmu_0721")
        self.ax1.grid(True, alpha=0.3)
        (self.score_line,) = self.ax1.plot([], [], "g-", linewidth=2)
        self.ax1.axhline(y=0.5, color="r", linestyle="--", label="é–¾å€¼")
        self.ax1.legend()
        
        # éŸ³é »æ³¢å½¢åœ–
        self.ax2.set_xlabel("æ¨£æœ¬")
        self.ax2.set_ylabel("æŒ¯å¹…")
        self.ax2.set_title("éŸ³é »æ³¢å½¢ (æœ€è¿‘1ç§’)")
        self.ax2.grid(True, alpha=0.3)
        (self.audio_line,) = self.ax2.plot([], [], "b-", alpha=0.7)
        # èª¿æ•´ y è»¸ç¯„åœä»¥é©æ‡‰å¯¦éš›çš„éŸ³é »æ•¸æ“šç¯„åœï¼ˆç´„ -200 åˆ° 200ï¼‰
        self.ax2.set_ylim(-500, 500)
        
        plt.tight_layout()
        
        # å•Ÿå‹•éŸ³é »è™•ç†ç·šç¨‹
        self.is_running = True
        self.last_detection_time = 0  # è¨˜éŒ„ä¸Šæ¬¡æª¢æ¸¬æ™‚é–“
        self.detection_cooldown = 0.8  # å†·å»æœŸ 0.8 ç§’
        audio_thread = threading.Thread(target=self.audio_callback)
        audio_thread.start()
        
        # å•Ÿå‹•å‹•ç•«
        ani = FuncAnimation(self.fig, self.update_plot, interval=50, blit=True)
        
        try:
            plt.show()
        except KeyboardInterrupt:
            print("\nçµæŸè¦–è¦ºåŒ–")
        finally:
            self.is_running = False
            audio_thread.join()
            self.p.terminate()
        
        # é¡¯ç¤ºçµ±è¨ˆ
        if self.state:
            model_name = list(self.state.keys())[0]
            scores = list(self.state[model_name])
            if scores:
                print(f"\n=== çµ±è¨ˆè³‡è¨Š ===")
                print(f"æ¨¡å‹: {model_name}")
                print(f"æœ€é«˜åˆ†æ•¸: {max(scores):.3f}")
                print(f"å¹³å‡åˆ†æ•¸: {np.mean(scores):.3f}")
                # è¨ˆç®—å¯¦éš›çš„å–šé†’è©æª¢æ¸¬æ¬¡æ•¸ï¼ˆè€ƒæ…®å†·å»æœŸï¼‰
                detection_count = 0
                last_detection = -self.detection_cooldown
                for i, s in enumerate(scores):
                    if s > 0.5:
                        # è¨ˆç®—æ™‚é–“å·®ï¼ˆå‡è¨­æ¯å¹€ç´„ 50msï¼‰
                        time_diff = (i - last_detection) * 0.05
                        if time_diff >= self.detection_cooldown:
                            detection_count += 1
                            last_detection = i
                
                print(f"è¶…éé–¾å€¼æ¬¡æ•¸: {sum(1 for s in scores if s > 0.5)} (åŸå§‹)")
                print(f"å¯¦éš›æª¢æ¸¬æ¬¡æ•¸: {detection_count} (è€ƒæ…®å†·å»æœŸ)")


def main():
    visualizer = WakeWordVisualizer()
    visualizer.run()


if __name__ == "__main__":
    main()